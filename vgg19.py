
HACKS = {'DEBUG_TRAIN':False,
'test=train' : False,
'DEBUG_TEST':False,
'IGNORE_SOFT_LINK':True}
# -*- coding: utf-8 -*-
"""running kaiyang zhou center loss with VGG19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/126G4jt2zo1Qqi58ANW3gQHBAkYjSEkYL

todo:
- [ ] change model
- [ ] change image size
- [x] train test split
"""

if not 'COLLAB':
    #!pip install -I --no-cache-dir pillow
    pass
if 'local':
#     %cd /home/aniketsingh/code/dump
    pass

#!git clone https://github.com/KaiyangZhou/pytorch-center-loss.git
# %cd pytorch-center-loss

#!wget -nc http://vis-www.cs.umass.edu/lfw/lfw.tgz
#!tar -xzf lfw.tgz

import os
import sys
import argparse
import datetime
import time
import os.path as osp
import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt
import numpy as np

import torch
import torch.nn as nn
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn

import datasets
import models
from utils import AverageMeter, Logger
from center_loss import CenterLoss
from torch.utils.data import DataLoader
import transforms
import torchvision
import tqdm 
import gc
import torch.nn as nn
from torch.nn import functional as F
import math
import pickle
from skimage import io
from PIL import Image
import lfw_splits
import os,collections

#-------------------------------------------------------------------------------
''' get the folder structure '''
def get_folder_structure(train_folder):
    folder_structure = collections.OrderedDict({})
    #HACK
    
    classes = [d for d in sorted(os.listdir(train_folder)) if  d not in ['.','..']]
    # if test_mode:
    # #     classes = [classes[c] for c in [2,5]]
    #     classes = classes[:10]
    try:
        listdir = os.listdir
        os.listdir(os.path.join(train_folder,classes[0]))
    except:
        listdir = lambda t: os.listdir(os.readlink(t))

    print(len(classes))
    folder_structure = collections.OrderedDict({c:[f for f in listdir(os.path.join(train_folder,c))] for c in classes})
    if False:print(folder_structure)
    #-------------------------------------------------------------------------------
    ''' which classes have which files '''
    class_to_idx = {k:[] for k in folder_structure.keys()}
    filelist = []
    i = 0
    for ( c ,fls) in folder_structure.items():   
        
        for fi in fls:
            filelist.append('/'.join([c,fi]))
            class_to_idx[c].append(i)
            i+=1    
    return folder_structure,classes,class_to_idx,filelist
folder_structure,classes,class_to_idx,filelist = get_folder_structure('lfw')
# if True:print(class_to_idx,filelist)
#-------------------------------------------------------------------------------

''' --------------- Transforms --------------- '''

from torch.utils.data import DataLoader
import transforms
import torchvision
from torch.utils.data.sampler import SubsetRandomSampler

mean = [0.5,0.5,0.5]
std = [0.5,0.5,0.5]
resize = torchvision.transforms.Resize(224)

aug = torchvision.transforms.Compose([
    torchvision.transforms.RandomCrop((224,224)),
    torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.5, hue=0.05),
    torchvision.transforms.RandomHorizontalFlip(p=0.5),
    torchvision.transforms.RandomAffine((-5,5), translate=None, scale=(0.8,1.2), shear=None, resample=False, fillcolor=0),
    ])

tensor_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),
                                  torchvision.transforms.Normalize(mean=mean,
                                                                   std=std)])

train_transform = torchvision.transforms.Compose([resize,
    aug,
    tensor_transform])

test_transform = torchvision.transforms.Compose([resize,
    torchvision.transforms.CenterCrop((224,224)),
    tensor_transform])

''' --------------- A sampler for class imbalance problem --------------- '''
class ClassBalancedSampler(torch.utils.data.Sampler):
    def __init__(self,classes,class_to_image_idx,batch_size,weighted=True):
        class_sample_counts = np.zeros((len(classes),))
        for cix,c in enumerate(classes):
            class_sample_counts[cix] = len(class_to_idx[c]) + 1e-4
        if weighted:
            self.class_weights = 1./(class_sample_counts)
            self.class_weights = self.class_weights/self.class_weights.sum()
        else:
            self.class_weights = 1./n_classes_in_set * np.ones_like(class_sample_counts)
        self.classes = classes
        self.class_to_image_idx = class_to_image_idx
        self.batch_size = batch_size
        self.n_batches_per_epoch = int((sum(class_sample_counts)+self.batch_size-1)//self.batch_size)
        self.iter_ix = 0
    def __iter__(self):
        while True:

            sampled_c = np.random.choice(self.classes, size=self.batch_size,replace = True, p= self.class_weights)
            #import pdb;pdb.set_trace()
            image_idx = [np.random.choice(self.class_to_image_idx[c]) for c in sampled_c]
            np.random.shuffle(image_idx)
            self.iter_ix +=1
            yield iter(image_idx)
        pass
    def __len__(self):
        return self.n_batches_per_epoch 
        
''' --------------- For Testing --------------- '''
class ClassSampler(torch.utils.data.Sampler):
    def __init__(self,
                 class_to_idx,
                 filelist,
                 ):

        self.class_to_idx = class_to_idx
        self.classes = list(self.class_to_idx.keys())
        self.n_classes = len(self.classes)
        
        self.filelist = filelist
        pass
    def __iter__(self):
        for c in self.classes:
            files = self.class_to_idx[c]
            yield files        
        pass

    def __len__(self):
        return self.n_classes
        pass
''' --------------- LFW dataset  --------------- '''
class LFWDataset(torch.utils.data.Dataset):
    def __init__(self,rootdir,filelist,setnames,transform):
        super(LFWDataset,self).__init__()
        self.filelist = filelist
        self.rootdir = rootdir
        self.transform = transform
        self.setnames = setnames
    def __getitem__(self,idx):
        
        fname = os.path.join(self.rootdir,
                        self.filelist[idx])
        d,f = os.path.split(fname)
        if os.path.islink(d):
            dreal = os.readlink(d)
            fname = os.path.join(dreal,f)
        label = self.filelist[idx].split('/')[0]

        image = io.imread(fname)
        pil_image = Image.fromarray(image)
        tensor_image = self.transform(pil_image)
        return tensor_image,torch.tensor(self.setnames.index(label))
        pass
    def __len__(self):
        return len(self.filelist)
        pass
''' --------------- Make LFW dataloaders --------------- '''
class LFWDataloaders(object):
    def __init__(self, batch_size, use_gpu, num_workers,train_transform,test_transform):

        pin_memory = True if use_gpu else False

        

        #trainset = torchvision.datasets.ImageFolder(root='./lfw', transform=train_transform)
        #testset = torchvision.datasets.ImageFolder(root='./lfw', transform=test_transform)

        train_dir = os.path.join(args.soft_dir,'train')
        test_dir = os.path.join(args.soft_dir,'test')

        train_folder_structure,train_classes,train_class_to_idx,train_filelist = get_folder_structure(train_dir)
        test_folder_structure,test_classes,test_class_to_idx,test_filelist = get_folder_structure(test_dir)
        


        trainset = LFWDataset(train_dir,train_filelist,train_classes,train_transform)# need custom Dataset to do balanced sampling
        testset = LFWDataset(test_dir,test_filelist,test_classes,test_transform)


        # based on https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb
        #train_sampler = SubsetRandomSampler(train_idx)
        train_sampler = ClassBalancedSampler(train_classes,train_class_to_idx,batch_size)
        #test_sampler = ClassBalancedSampler(test_classes,test_class_to_idx,batch_size,weighted=False)
        test_classwise_sampler = ClassSampler(test_class_to_idx,test_filelist)


        
        #testloader = torch.utils.data.DataLoader(
        #    testset, 
        #    num_workers=num_workers, pin_memory=pin_memory,
        #) # test doesnt need a sampler

        trainloader = torch.utils.data.DataLoader(
                trainset, batch_sampler = train_sampler,
                num_workers=num_workers, pin_memory=pin_memory,
            ) # test doesnt need a sampler
            
        testloader1 = torch.utils.data.DataLoader(
                testset, batch_sampler = test_classwise_sampler,
                num_workers=num_workers, pin_memory=pin_memory,
            ) # test doesnt need a sampler

        testloader2 = torch.utils.data.DataLoader(
                testset, batch_sampler = test_classwise_sampler,
                num_workers=num_workers, pin_memory=pin_memory,
            ) # test doesnt need a sampler

        self.trainloader = trainloader
        self.testloader1 = testloader1
        self.testloader2 = testloader2
#         self.num_classes = 5749
        self.n_train_classes = len(train_classes)

#--------------------------------


'''---------------  Make the arguments for the run --------------- '''

parser = argparse.ArgumentParser("Center Loss Example")
# dataset
# parser.add_argument('-d', '--dataset', type=str, default='mnist', choices=['mnist'])
parser.add_argument('-d', '--dataset', type=str, default='lfw')
parser.add_argument('-j', '--workers', default=4, type=int,
                    help="number of data loading workers (default: 4)")
# optimization
parser.add_argument('--batch-size', type=int, default=64)
parser.add_argument('--lr-model', type=float, default=0.0001, help="learning rate for model")
parser.add_argument('--lr-cent', type=float, default=0.5, help="learning rate for center loss")
parser.add_argument('--weight-cent', type=float, default=1, help="weight for center loss")
parser.add_argument('--max-epoch', type=int, default=100)
parser.add_argument('--stepsize', type=int, default=20)
parser.add_argument('--gamma', type=float, default=0.5, help="learning rate decay")
# model
parser.add_argument('--model', type=str, default='cnn')
# misc
parser.add_argument('--eval-freq', type=int, default=1)#10
parser.add_argument('--print-freq', type=int, default=25)
parser.add_argument('--gpu', type=str, default='1')
parser.add_argument('--seed', type=int, default=1)
# parser.add_argument('--use-cpu', action='store_false')
parser.add_argument('--save-dir', type=str, default='log')
parser.add_argument('--plot', action='store_false', help="whether to plot features for every epoch")
parser.add_argument('--model_save_path', type=str, default = 'trained_models')
parser.add_argument('--soft_dir', type=str,default = 'lfw_divided')


#args = parser.parse_args(['--dataset','lfw','--gpu','0'])

args = parser.parse_args()
args.use_cpu = False
args.plot = True
args.embed_size = 32
args.use_normed = True
args.create_splits = True
#args.soft_dir = 'lfw_divided'
# args = parser.parse_args(['--plot','false'])

'''---------------  split lfw into directories --------------- '''
if args.create_splits:
    train_ratio = 0.8
    splits = lfw_splits.create_splits(classes,train_ratio)
else:
    with open('lfw_splits','rb') as f:
        splits = pickle.load(f)
if not HACKS['IGNORE_SOFT_LINK']:
    lfw_splits.create_soft_dir('lfw',args.soft_dir,splits)
print('soft links created')


'''for saving'''
model_savepath = args.model_save_path
model_name = 'test_model'
if not os.path.isdir(model_savepath):
    os.makedirs(model_savepath)
save_every_n_epochs = 10

'''---------------  some prerequisites: reproducibility, gpu etc. --------------- '''

torch.manual_seed(args.seed)
#os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
use_gpu = torch.cuda.is_available()
if args.use_cpu: use_gpu = False

sys.stdout = Logger(osp.join(args.save_dir, 'log_' + args.dataset + '.txt'))

if use_gpu:
    print("Currently using GPU: {}".format(args.gpu))
    cudnn.benchmark = True
    torch.cuda.manual_seed_all(args.seed)
else:
    print("Currently using CPU")

'''---------------  Get the DataLoaders --------------- '''

print("Creating dataset: {}".format(args.dataset))

if args.dataset.lower() == 'mnist':
    dataset = datasets.create(
        name=args.dataset, batch_size=args.batch_size, use_gpu=use_gpu,
        num_workers=args.workers,)

#     dataset = MNIST(batch_size=args.batch_size, use_gpu=use_gpu,
#     num_workers=args.workers,)
    trainloader, testloader = dataset.trainloader, dataset.testloader
    num_classes = dataset.num_classes
elif args.dataset.lower() == 'lfw':

    with open('lfw_splits','rb') as f:
      splits = pickle.load(f)
    dataset = LFWDataloaders(batch_size=args.batch_size, use_gpu=use_gpu,
    num_workers=args.workers,train_transform=train_transform,test_transform=test_transform)
    trainloader, testloader1,testloader2 = dataset.trainloader, dataset.testloader1, dataset.testloader2
    num_classes = dataset.n_train_classes

'''--------------- Make the model --------------- '''

#    

print("Creating model: {}".format(args.model))
if args.dataset.lower() == 'mnist':
    model = models.create(name=args.model, num_classes=dataset.num_classes)#ConvNet(num_classes=num_classes) 
elif args.dataset.lower() == 'lfw':
    
    model = torchvision.models.vgg19(pretrained=True)

    in_ =list(model.classifier[3].parameters())[0].shape[-1]
    new_fc = torch.nn.Linear(in_,args.embed_size)
    model.classifier[3] = new_fc

    out_ = num_classes
    new_out = torch.nn.Linear(args.embed_size,num_classes)
    
    model.classifier[-1] = new_out
    ''' To get the intermediate features '''
    #--------------------------------------------
    '''
    def make_fwd_hook(store):
        def fwd_hook(self,input,output):
            gpu_ix = output.get_device()
            store[gpu_ix] = output
            pass
        return fwd_hook
    embeddings = [None]*torch.cuda.device_count()
    last_fcs = [None]*torch.cuda.device_count()
    _ = model.classifier[-3].register_forward_hook(make_fwd_hook(embeddings))
    _ = model.classifier[-1].register_forward_hook(make_fwd_hook(last_fcs))
    '''
    #----------------------------------------------
    
    def fwd_hook(self,input,output):
       self.feat = output
       pass
    _ = model.classifier[-3].register_forward_hook(fwd_hook)
    _ = model.classifier[-1].register_forward_hook(fwd_hook)
    
   #----------------------------------------------

if use_gpu:
    #model = nn.DataParallel(model)
    model = model.cuda()
    pass


'''--------------- testing --------------- '''
import sklearn.metrics
def test(model, testloader1,testloader2, use_gpu, num_classes, epoch,d_thresh = np.arange(0.,2.,0.1)):
    model.eval()

    classwise_dist = []
    _ = list(testloader1.batch_sampler.class_to_idx.values())
    classwise_numels = [len(c) for c in _]
    n_classes = len(testloader1.dataset.setnames)    
    mean_cluster_d = 0.
    max_cluster_d = 0.
    with torch.no_grad():
        for i,(data, labels) in enumerate(testloader1):
            if use_gpu:
                data, labels = data.cuda(), labels.cuda()

            c_numel = classwise_numels[i]
            _ = model(data)
            features = model.classifier[-3].feat 
            outputs = model.classifier[-1].feat

            feature_normed = features.div(
            torch.norm(features, p=2, dim=1, keepdim=True).expand_as(features))
 
            feature_normed_np = feature_normed.cpu().detach().numpy()
            feature_normed_np1 = feature_normed_np
            D_ij = sklearn.metrics.pairwise.euclidean_distances(feature_normed_np1,
                                     feature_normed_np1)
            D_ij = np.triu(D_ij,1).flatten()
            
            class_mean_d = np.mean(D_ij)
            mean_cluster_d += class_mean_d       
            max_cluster_d = max(max_cluster_d,class_mean_d)

            ################################################

            if HACKS['DEBUG_TEST']:
                break
        mean_cluster_d = mean_cluster_d/(i+1)
    return mean_cluster_d,max_cluster_d

'''--------------- Prepare the criterions --------------- '''

criterion_xent = nn.CrossEntropyLoss()
criterion_cent = CenterLoss(num_classes=num_classes, feat_dim=args.embed_size, use_gpu=use_gpu)

'''--------------- Prepare the optimizers --------------- '''
optimizer_model = torch.optim.SGD(model.parameters(), lr=args.lr_model, weight_decay=5e-04, momentum=0.9)
#optimizer_centloss = torch.optim.SGD(criterion_cent.parameters(), lr=args.lr_cent)

#optimizer_model = torch.optim.Adam(model.parameters())
optimizer_centloss = torch.optim.SGD(criterion_cent.parameters(), lr=args.lr_cent)

if args.stepsize > 0:
    scheduler = lr_scheduler.StepLR(optimizer_model, step_size=args.stepsize, gamma=args.gamma)

#outputs.shape,features.shape,labels.shape

'''--------------- training loop --------------- '''

start_time = time.time()

for epoch in tqdm.tqdm(range(args.max_epoch)):
    print("==> Epoch {}/{}".format(epoch+1, args.max_epoch))
    
    model.train()
    xent_losses = AverageMeter() 
    cent_losses = AverageMeter()
    losses = AverageMeter()
    
    if args.plot:
        all_features, all_labels = [], []

    for batch_idx, (data, labels) in enumerate(trainloader):
        print(batch_idx,)
        #-----------------------------------------------------------------------
        # forward pass
        
        if use_gpu:
            data, labels = data.cuda(), labels.cuda()
        if args.dataset.lower() == 'mnist':
            features, outputs = model(data)
        else:
            _ = model(data)
            features = model.classifier[-3].feat 
            outputs = model.classifier[-1].feat

        #-----------------------------------------------------------------------
        # losses
        
        #print(outputs.shape,features.shape,labels.shape)
        loss_xent = criterion_xent(outputs, labels)
        if args.use_normed:
            feature_normed = features.div(
            torch.norm(features, p=2, dim=1, keepdim=True).expand_as(features))

            loss_cent = criterion_cent(feature_normed, labels)
        else:
            loss_cent = criterion_cent(features, labels)

        loss_cent *= args.weight_cent
        loss = loss_xent + loss_cent
        #-----------------------------------------------------------------------
        # backward pass
        
        optimizer_model.zero_grad()
        optimizer_centloss.zero_grad()
        loss.backward()
        optimizer_model.step()
        # by doing so, weight_cent would not impact on the learning of centers
        for param in criterion_cent.parameters():
            param.grad.data *= (1. / args.weight_cent)
        optimizer_centloss.step()

        #-----------------------------------------------------------------------
        # bookkeeping
        
        losses.update(loss.item(), labels.size(0))
        xent_losses.update(loss_xent.item(), labels.size(0))
        cent_losses.update(loss_cent.item(), labels.size(0))

        #-----------------------------------------------------------------------
        # plotting
        
        if args.plot:
            if use_gpu:
                all_features.append(features.data.cpu().numpy())
                all_labels.append(labels.data.cpu().numpy())
            else:
                all_features.append(features.data.numpy())
                all_labels.append(labels.data.numpy())

        if (batch_idx+1) % args.print_freq == 0:
            print("Batch {}\t Loss {:.6f} ({:.6f}) XentLoss {:.6f} ({:.6f}) CenterLoss {:.6f} ({:.6f})" \
                  .format(batch_idx+1,  losses.val, losses.avg, xent_losses.val, xent_losses.avg, cent_losses.val, cent_losses.avg))
        #-----------------------------------------------------------------------
        # garbage collection
        gc.collect()
        if HACKS['DEBUG_TRAIN'] and batch_idx>2:
            break
        if batch_idx>len(trainloader):
            break

   
    #-----------------------------------------------------------------------
    # scheduler step
    if args.stepsize > 0: scheduler.step()

    #-----------------------------------------------------------------------
    # testing phase        
    if args.eval_freq > 0 and (epoch+1) % args.eval_freq == 0 or (epoch+1) == args.max_epoch:
        print("==> Test")
        mean_cluster_d = 0
        mean_cluster_d,max_cluster_d = test(model, testloader1,testloader2, use_gpu, num_classes, epoch)
        print(f'mean_cluster_d {mean_cluster_d} max_cluster_d {max_cluster_d}')
        #-----------------------------------------------------------------------
        # saving
    if save_every_n_epochs%(epoch+1) == 0:
        save_params = { 
            'epoch': epoch,
            'model_sd': model.state_dict(),
            'optimizer_model_sd': optimizer_model.state_dict(),
            'scheduler_sd' : scheduler.state_dict(),
            'mean_cluster_d': mean_cluster_d,

            }
        torch.save(save_params,model_savepath+model_name+str(epoch)+'.th')
        

#-----------------------------------------------------------------------
# elaspsed time        
elapsed = round(time.time() - start_time)
elapsed = str(datetime.timedelta(seconds=elapsed))
print("Finished. Total elapsed time (h:m:s): {}".format(elapsed))

'''--------------- XXXXX  --------------- '''




